# ğŸ§ Weekly Spotify Wrapped Pipeline

## âœ¨ Project Overview
This project demonstrates an end-to-end Machine Learning Engineering pipeline designed to generate personalized "Weekly Spotify Wrapped" insights. It leverages real-time data streaming to process listening history, store it, and utilize Generative AI to provide unique weekly recaps and recommendations. Inspired by the popular Spotify Wrapped feature, this pipeline aims to offer continuous insights into music listening habits without waiting for a year.

## ğŸš€ Key Features
-   **Spotify Data Ingestion:** Fetches recently played tracks from Spotify API via a Flask web service.
-   **Real-time Data Streaming:** Utilizes **Apache Kafka** as a messaging system to handle real-time data streams (songs, albums, artists, history).
-   **Stream Processing & Data Warehousing:** Employs **Apache Spark Streaming** to process and analyze real-time data, then stores it in a **PostgreSQL** database following a Star Schema (Fact and Dimension tables).
-   **AI-Powered Insights:** Integrates **Google Gemini AI** to generate personalized weekly recap summaries, mood recaps, and music recommendations based on listening history.
-   **Email Delivery:** Delivers AI-generated insights directly to the user's email.
-   **Automated Orchestration (Planned):** Designed for weekly scheduling via external tools (e.g., Airflow, cron, iOS automation).

## ğŸ’» Tech Stack
-   **Main Programming Language:** Python
-   **Web Service:** Flask (for Spotify API integration and insight endpoints)
-   **Messaging System:** Apache Kafka (for real-time data streams)
-   **Stream Processing:** Apache Spark Streaming
-   **Database:** PostgreSQL (for structured data storage, Star Schema)
-   **Generative AI:** Google Gemini AI
-   **Containerization:** Docker (for Kafka cluster setup)
-   **ORM:** SQLAlchemy (for database interaction)

## âš™ï¸ Project Structure 
```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ .env                # Environment variables (sensitive data)
â”œâ”€â”€ requirements.txt    # Python package dependencies
â”œâ”€â”€ flask_backend       # Flask web service for API calls & insights
â”‚   â””â”€â”€ app.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ kafka-clusters      # Docker Compose setup for Kafka & Control Center
â”‚   â””â”€â”€ docker-compose.yml
â””â”€â”€ prod-con            # Producer, Consumer, & Spark Streaming scripts
â”œâ”€â”€ client.properties
â”œâ”€â”€ models.py
â”œâ”€â”€ producer.py
â”œâ”€â”€ spark-consumer-album.py
â”œâ”€â”€ spark-consumer-artist.py
â”œâ”€â”€ spark-consumer-history.py
â””â”€â”€ spark-consumer-song.py
```

## ğŸš€ How to Set Up & Run
*(Note: This is a high-level guide. Refer to individual script comments for detailed setup.)*

1.  **Clone the Repository:**
    ```bash
    git clone [https://github.com/your-username/weekly-spotify-wrapped-pipeline.git](https://github.com/your-username/weekly-spotify-wrapped-pipeline.git) # Ganti dengan link repo kamu
    cd weekly-spotify-wrapped-pipeline
    ```
2.  **Environment Setup:**
    * Create and activate a Python virtual environment.
    * Install dependencies: `pip install -r requirements.txt`
3.  **API Keys & Credentials:**
    * Obtain Spotify Client ID/Secret, Gemini API Key, Google App Password.
    * Configure PostgreSQL database details.
    * Update these in your `.env` file (root directory).
4.  **Kafka Broker Setup:**
    * Navigate to `kafka-clusters/` and run `docker compose up -d --build`.
    * Create Kafka topics: `album_topic`, `artist_topic`, `song_topic`, `item_topic` via Confluent Control Center (`http://127.0.0.1:9021`).
5.  **Run Flask Web Service:**
    * Navigate to `flask_backend/` and run `flask run --reload`.
    * Access `http://127.0.0.1:5000/login` to authenticate with Spotify.
6.  **Run Producer & Consumers:**
    * Start the producer: `python prod-con/producer.py`
    * Start each Spark consumer (album, artist, song, history) in separate terminals (requires Apache Spark installation & JDBC driver).

## ğŸ’¡ Future Enhancements
-   Implement a unified Spark consumer for all topics.
-   Add data quality checks within the Spark streaming process.
-   Containerize the entire pipeline (Flask, Producer, Consumers) using Docker Compose for easier deployment.
-   Set up dedicated orchestration for the AI insight generation (e.g., Apache Airflow).
-   Integrate with a cloud-based data platform (e.g., AWS S3, EMR, Kinesis).

## ğŸ“„ License
This project is open-source under the MIT License.
